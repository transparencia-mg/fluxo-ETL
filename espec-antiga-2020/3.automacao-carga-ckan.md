## 3. Descrição dos Fluxos

## Comportamento geral

- Cada ambiente do CKAN possui um branch associado no repositório git. O ambiente de homologação está associado ao branch `homologa` e o ambiente de produção ao branch `prod`

- O primeiro commit no branch `homologa` gera a criação de um novo conjunto de dados na instância do CKAN em homologação

- O primeiro commit no branch `prod` gera a criação de um novo conjunto de dados na instância do CKAN em ambiente de produção

- O processo inicial de criação deve popular:
	 
	 os metadados do conjunto de dados; 

	 os metadados dos recursos;

	 os metadados do dicionário de dados (Data Store), caso existam;

	 os dados do recurso

- Todo datapackage deve ter pelo menos um recurso (Na especificação do frictionless data, é obrigatório que o data package tenha propriedade fazendo referência ao recurso.)

- O processo de criação nos dois ambientes deve validar que o arquivo `datapackage.json` existe e é um data package válido do ponto de vista estrutural (sintaxe)
    - Exemplo: Vírgula faltando no arquivo `json`

- Após a criação inicial, todo commit nos branchs `homologa` e `prod` deve verificar a necessidade de mudança de metadados, em pelo menos algum dos 3 níveis (conjunto, recurso ou dicionário). 

- As seguintes mudanças no datapackage.json deverão ser refletidas no ambiente apropriado do CKAN:

	+ Atualizar metadados datapackage
    + Atualizar metadados dos recursos
    + Atualizar metadados do dicionário de dados (modificação de propriedades, adição ou exclusão de variáveis)
    + Adicionar recursos
    + Excluir recursos

- A necessidade de atualização de metadados deve ser realizada pelo confronto entre a propriedade `version` do `datapackage.json` e a propriedade `version` do conjunto de dados no CKAN. A mudança de versão deve atualizar todos os metadados 


## Data packages com armazenamento no repositório 

- Após a criação inicial, todo commit nos branches `homologa` e `prod` deve gerar atualização dos recursos alterados da pasta `data`. Essa informação está disponível no histórico do commit. O commit é um evento, que tem de ser interpretado para determinar a ação a ser refletida no conjunto de dados. As mudanças podem ser relativas a:

- Commit (evento) - Após commit inicial
    - Evento deve ser interpretado pela rotina para determinação da mudança que deve ser refletida no conjunto de dados
    - As mudanças podem ser relativas à:
        + Atualizar metadados datapackage
        + Atualizar metadados dos recursos
        + Atualizar metadados do dicionário de dados
        + Adicionar recursos
        + Excluir recursos
    - Atualizar dados dos recursos

- Para cada job de execução, deve haver um log acessível e perene no tempo com:
	- identificador, 
	- resultado de validação (sintaxe/estrutura arquivos)

- Se houver algum desvio nos comportamentos descritos nesta especificação, uma notificação deve ser enviada para o email determinado contendo a listagem de jobs com erro

#### Carga

0. Etapa prévia à publicação

O administrador do portal validará a versão final do arquivo junto ao custodiante do dado, cotejando aspectos de metadados, recursos e versionamento mencionados anteriormente. 

Validada a versão final do arquivo para publicação, ela deverá ser incluída (commitada) no repositório correspondente no Github, seguindo as regras e convenções de nomenclatura e estrutura de pastas e arquivos (incluindo o caminho relativo indicado na propriedade _path_: **data/arquivo.csv**). 

É necessário que os arquivos de recursos estejam em conformidade com os valores das propriedades do `datapackage.json` (validação de que o arquivo físico deve estar igual ao descrito). As inconsistências devem ser abordadas e resolvidas até o horário diário predeterminado de atualização do CKAN pelo script de carga.


1. Arquivo pronto para publicação:

É necessário que o script de carga tenha uma função para perceber, diariamente, se alterações foram realizadas no `datapackage.json` e/ou nos recursos de cada dataset. 

OU

A comunicação da mudança de versão do `datapackage.json` e da versão dos arquivos é um processo externo; quando o nome do arquivo existir na listagem atual do dataset, o recurso deve ser atualizado (e o `datapackage.json`, se houver mudança de propriedade ou mudança de valor de propriedade)

As alterações devem ser refletidas no ambiente em produção, numa publicação automatizada. Para tal, a rotina deve realizar a importação dessa última versão commitada dos arquivos alterados no respectivo repositório para o ftp de publicação da máquina CKAN do administrador de sistemas. Na inviabilidade de importação dos arquivos pelo script de carga, o adminsitrador do Portal fará o upload manual dos mesmos para o ftp de publicação da máquina CKAN, utilizando a mesma estrutura de pastas e arquivos mencionada anteriormente. Em qualquer dos casos, a rotina fará o upload dos arquivos alterados e atualização dos campos de metadados (chaves/keys e respectivos valores) no CKAN, de modo que fiquem visíveis na interface gráfica, sem necessidade de inserção manual.

A necessidade de atualização dos metadados será indicada pelo valor da propriedade versão no datapackage.json

* Situações:

1.1. alteração somente de datapackage.json

1.2. adição ou supressão de recurso 

1.3. alteração de recurso:

	propriedades do recurso

	valores das variáveis

	propriedades das variáveis



2. Pós-publicação (carga/upload)

Ao realizar a publicação, é necessário que o script atualize o dicionário de dados automátizado da extensão DataStore, para que os metadados das variáveis (título, tipo, descrição representem as alterações realizadas com o upload do novo arquivo). Na inviabilidade dessa atualização automática, o relatório de notificação descrito a seguir deve ter incluída uma mensagem de aviso ou lembrete para a necessidade de atualização manual do Dicionário de Dados.

Ao fim do processo de publicação, é necessário que o script gere um relatório de notificação de erros resumidos para os emails previamente registrados dos agentes do adminsitrador do Portal. Tal relatório deve resumir e quantificar as operações realizadas na rotina, precisando sua data e horário de realização, e também deve permitir o acesso um relatório mais detalhado (log) que compare os resultados (outputs) das operações previstas com aquelas realizadas (modelo ?).

Caso exista duplicação de dados para publicação dos mesmos no CKAN, o serviço de extração e carga deve:

* Garantir a consistência entre os dados quando existirem alterações na base de dados origem;
* Utilizar um usuário específico para carga no CKAN;
* Configurável para efetuar cargas em instâncias CKAN não hospedadas na PRODEMGE;
* Possuir mecanismo de monitoramento das cargas realizadas (eg. email com log de atualização)

#### Processamento

 Arquivo tem que passar por processamento para ser publicado:


 Arquivo primário tem que ser processado mas não pode ser divulgado (ex.: unidade administrativa)
